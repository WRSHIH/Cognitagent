import logging

from functools import lru_cache
from langchain.tools import Tool
from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.indices.base import BaseIndex

# å°å…¥é›†ä¸­ç®¡ç†çš„æœå‹™èˆ‡è¨­å®š
from core.services import get_qdrant_client, configure_llama_index_settings, get_llama_gemini_flash
from core.config import settings

# 1. é€£æ¥åˆ°å·²å­˜åœ¨çš„ Vector Store
@lru_cache(maxsize=None)
def get_vector_store():
    logging.info('é¦–æ¬¡åˆå§‹åŒ– QdrantVectorStore...')
    return QdrantVectorStore(
        client=get_qdrant_client(),
        collection_name="my_KnowledgeBase",
        enable_hybrid=True,
    )

# 2. å¾ Vector Store å¯¦ä¾‹åŒ–ç´¢å¼•ç‰©ä»¶
@lru_cache(maxsize=None)
def get_index() -> BaseIndex:
    logging.info('é¦–æ¬¡åˆå§‹åŒ– VectorStoreIndex...')
    configure_llama_index_settings()
    return VectorStoreIndex.from_vector_store(vector_store=get_vector_store())

# 3. å»ºç«‹æŸ¥è©¢å¼•æ“
def run_deep_research(query: str) -> str:
    logging.info("å‘é‡è³‡æ–™åº«æ¢ç´¢...")
    index = get_index()
    query_engine = index.as_query_engine(
                    llm = get_llama_gemini_flash(),
                    similarity_top_k=settings.SIMILARITY_TOP_K,
                    sparse_top_k=settings.SPARSE_TOP_K,
                    vector_store_query_mode=settings.QUERY_MODE,
                    alpha=settings.HYBRID_SEARCH_ALPHA,
                    num_queries = settings.NUM_QUERIES,
                    node_postprocessors = settings.POSTPROCESSORS,
                    streaming = settings.STREAMING,
                )
    result = query_engine.query(query)
    return str(result)

# 4. å®šç¾©ä¸¦å°å‡ºå·¥å…·
DeepResearchKnowledgeBase = Tool(
    name="DeepResearchKnowledgeBase",
    func=run_deep_research,
    description="""ä¸€å€‹æ·±åº¦ç ”ç©¶å·¥å…·ï¼Œå°ˆé–€ç”¨æ–¼æŸ¥è©¢å’Œå›ç­”é—œæ–¼ã€Œè‡¨åºŠè©¦é©—è¨­è¨ˆã€ã€ã€Œè—¥ç‰©é–‹ç™¼æµç¨‹ã€ã€ä»¥åŠã€ŒGCPè¦ç¯„ã€ç­‰å°ˆæ¥­é ˜åŸŸçš„å…§éƒ¨çŸ¥è­˜åº«ã€‚
                 ç•¶ä½¿ç”¨è€…çš„å•é¡Œæ¶‰åŠè‡¨åºŠè©¦é©—çš„å…·é«”åŸ·è¡Œç´°ç¯€ã€æ³•è¦éµå¾ªæˆ–å…§éƒ¨æµç¨‹æ™‚ï¼Œå¿…é ˆå„ªå…ˆä½¿ç”¨æ­¤å·¥å…·ã€‚"""
)

if __name__ == "__main__":
    # è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸ (API é‡‘é‘°ç­‰)
    # è«‹ç¢ºä¿æ‚¨å·²å®‰è£ python-dotenv: pip install python-dotenv
    try:
        from dotenv import load_dotenv
        print("æ­£åœ¨å¾ .env æª”æ¡ˆè¼‰å…¥ç’°å¢ƒè®Šæ•¸...")
        load_dotenv()
        print("è¼‰å…¥å®Œæˆã€‚")
    except ImportError:
        print("æœªæ‰¾åˆ° python-dotenvï¼Œè«‹ç¢ºä¿æ‚¨çš„ API é‡‘é‘°å·²æ‰‹å‹•è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸ã€‚")

    # è¨­å®šæ—¥èªŒï¼Œä»¥ä¾¿æˆ‘å€‘èƒ½çœ‹åˆ°å‡½å¼å…§éƒ¨çš„ INFO è¨Šæ¯
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # å®šç¾©ä¸€å€‹ç¬¦åˆå·¥å…·æè¿°çš„æ¸¬è©¦å•é¡Œ
    TEST_QUERY = "è‡¨åºŠè©¦é©—çš„ GCP è¦ç¯„æ˜¯ä»€éº¼ï¼Ÿ"
    
    print("\n" + "="*50)
    print(f"ğŸš€ é–‹å§‹é©—è­‰ DeepResearchKnowledgeBase å·¥å…·é€£ç·š...")
    print(f"æ¸¬è©¦å•é¡Œ: {TEST_QUERY}")
    print("="*50 + "\n")

    try:
        # ç›´æ¥å‘¼å«æ ¸å¿ƒå‡½å¼ï¼Œé€™æœƒè§¸ç™¼æ‰€æœ‰ç›¸é—œçš„é€£ç·š
        # 1. get_qdrant_client() -> é€£ç·šåˆ° Qdrant
        # 2. configure_llama_index_settings() -> åˆå§‹åŒ– Embedding æ¨¡å‹ (éœ€è¦ Google API Key)
        # 3. get_llama_gemini_flash() -> åˆå§‹åŒ– LLM (éœ€è¦ Google API Key)
        result = run_deep_research(TEST_QUERY)
        
        print("\n" + "="*50)
        print("âœ… æ¸¬è©¦æˆåŠŸï¼æ‰€æœ‰é€£ç·šå‡æ­£å¸¸ã€‚")
        print("="*50)
        print("\næŸ¥è©¢çµæœé è¦½ï¼š")
        # ç‚ºäº†é¿å…è¼¸å‡ºéé•·ï¼Œåªé¡¯ç¤ºå‰ 500 å€‹å­—å…ƒ
        print(result[:500] + "..." if len(result) > 500 else result)

    except Exception as e:
        print("\n" + "="*50)
        print("âŒ æ¸¬è©¦å¤±æ•—ï¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ã€‚")
        print("="*50)
        # exc_info=True æœƒå°å‡ºå®Œæ•´çš„éŒ¯èª¤è¿½è¹¤å›æº¯ (Traceback)
        # é€™å°‡æ˜¯è§£æ±º UnexpectedResponse çš„æœ€é—œéµç·šç´¢
        logging.error("éŒ¯èª¤è©³æƒ…å¦‚ä¸‹:", exc_info=True)